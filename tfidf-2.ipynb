{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "90e72e5b-dfc7-4707-8226-b16c5b754bc1",
      "cell_type": "code",
      "source": "import ast\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import LabelEncoder",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "818b6066-59fe-458a-8c98-a1cc787fb37d",
      "cell_type": "markdown",
      "source": "## Fetching data",
      "metadata": {}
    },
    {
      "id": "0f62b1d3-e68e-4b2e-8274-7e5e3eae7f16",
      "cell_type": "code",
      "source": "data = pd.read_csv('../data/Hotel_Reviews.csv') #read the csv file",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "a24cbd57-f45a-443f-ad76-4d1ca59ac978",
      "cell_type": "markdown",
      "source": "## Data preprocessing",
      "metadata": {}
    },
    {
      "id": "79624fc7-c03a-4115-b545-8fd1f6ec8d41",
      "cell_type": "code",
      "source": "# This is the basic data cleaning process. This should be added in all the models\n# This cell is a bit slow to run, so it is intended to be run only once\n\n# This part drops the columns that are not needed in the model\ndata.drop('Hotel_Address',axis=1,inplace=True)  # Drop the column 'Hotel_Address' due to lack of relevance\ndata.drop('Review_Date',axis=1,inplace=True)    # Drop the column 'Review_Date' due to lack of complete data\ndata.drop('Additional_Number_of_Scoring',axis=1,inplace=True)  # Drop the column 'Additional_Number_of_Scoring' due to not knowing what the numbers mean\ndata.drop('lat',axis=1,inplace=True)    # Drop the column 'lat' due to lack of relevance\ndata.drop('lng',axis=1,inplace=True)    # Drop the column 'lng' due to lack of relevance\ndata.drop('Total_Number_of_Reviews',axis=1,inplace=True)    # Drop the column 'Total_Number_of_Reviews' due to the number seams to be incorrect\n\n# The next part of the code aims to split the 'Tags' column into multiple columns\ndata['Tags'] = data['Tags'].apply(ast.literal_eval) # Convert the string to a list, This code is made by chatgpt\n\n# The following 3 lines of code is made by github copilot\ntags_expanded = data['Tags'].apply(pd.Series) # Expand the 'Tags' column into multiple column\ntags_expanded.columns = [f'Tag_{i}' for i in range(tags_expanded.shape[1])] # Rename the columns for better readability\n\ndata = pd.concat([data, tags_expanded], axis=1) # Concatenate the expanded tags with the original dataframe\ndata.drop('Tags', axis=1, inplace=True)   # Drop the column 'Tags' due to the data being split into multiple columns\n\n# The next part of the code turns the 'days_since_review' and 'Tag_3' columns into integers\ndata['days_since_review'] = data['days_since_review'].str.extract('(\\d+)').astype(int) # Extract the number from the string. This code if made using chatgpt\ndata['Tag_3'] = data['Tag_3'].str.extract('(\\d+)').astype(float) # Extract the number from the string and convert to float. For some reason it did not work as int\n\n# The next part of the code creates a new dataframe with hotel names and removes it from the dataframe data\nhotel_names = data['Hotel_Name'] # Create a new dataframe with the hotel names\ndata.drop('Hotel_Name',axis=1,inplace=True) # Drop the column 'Hotel_Name' as it is not needed in the model\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "bcb782cf-ce98-458d-9dea-03e02a204227",
      "cell_type": "markdown",
      "source": "## Model 4: TF-IDF\nThe fourth and final model is based on feature extraction using the TF-IDF metric and modelling with logistic regression.\n\n### TF-IDF\n*Term frequency-inverse document frequency* is a measure of how unique a term is to a document in a corpus. This is calculated by finding the product of two measures, namely term frequency and inverse document frequency.\n\nTerm frequency is the number of times a term $t$ appears in a document $d$ divided by the total number of terms (including repeated terms). This can be denoted by the following function:\n\n$$tf(t,d)=\\frac{f_{t,d}}{\\sum_{t'\\in d} f_{t',d}}$$\n\nAs an example we look at the following 'document': *\"To be or not to be, that is the question.\"* We call that document $d$. Using the function above we find the following frequencies:\n\n$$tf(\\mathrm{\"be\"}, d)=\\frac{2}{10}=0.2$$\n$$tf(\\mathrm{\"is\"}, d)=\\frac{1}{10}=0.1$$\n$$tf(\\mathrm{\"question\"}, d)=\\frac{1}{10}=0.1$$\n\nFrom these results one could conclude that the words \"is\" and \"question\" bear the same importance in the document. Most would however say that \"question\" is a more defining word as it is more unique. This is where inverse term frequency comes in.\n\nInverse term frequency takes in to account the entire corpus $D$. It counts the number of documents in the corpus and divides it by the number of documents which contain the term $t$ and then takes the logarithm. This is denoted by the following function:\n\n$$idf(t,D)=\\log{\\frac{|D|}{|\\{d:d\\in D \\,\\wedge \\,t\\in d\\}|}}$$\n\nThis leads to less common terms getting a higher $idf$. This lets us calculate the TF-IDF metric of a term $t$ in a document $d$ of a corpus $D$:\n\n$$tfidf(t,d,D)=tf(t,d)\\cdot idf(t,D)$$\n\nTo set the dataset up for TF-IDF we do the following:",
      "metadata": {}
    },
    {
      "id": "63d0a9eb-e0fa-4673-946e-6977bfae4c23",
      "cell_type": "code",
      "source": "tfidf_data = data.copy() # Take a copy of the data\n\nreviews = tfidf_data[\"Negative_Review\"] # Extract the negative reviews.\n\ntfidf_vectorizer = TfidfVectorizer(stop_words=\"english\") # Set up the vectorizer class\n# NOTE: stop_words filters out common short words in english, e.g 'is, 'the', etc.\n\nX = tfidf_vectorizer.fit_transform(reviews) # Vectorize all reviews and assign as input variable.\ny = data[\"Reviewer_Score\"].map(lambda x: 0 if x < 5.0 else 1) # Generate correpsponding output.\n\nprint(X.get_feature_names_out())\nprint(X.head())",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "ccfa3cde-0b66-4cd0-a456-10c251885708",
      "cell_type": "markdown",
      "source": "The *TfidfVectorizer* creates a set of all words contained in all of the reviews. It then calculates a TF-IDF score for each unique word for each document in the corpus. This should allow us to create a model which can correlate the combination of frequencies of certain words to either a good or bad sentiment. \n\nAs the result can either be 'good' or 'bad', we could assign a probability to each of them which add up to 100%. A model based on logistic regresison would correspond to this scenario. We set it up using the built in class from Scikit.",
      "metadata": {}
    },
    {
      "id": "d663224f-dfe9-4325-a7d3-4a22a84ac7f5",
      "cell_type": "code",
      "source": "# Create train/test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Set up Logistic Regression model.\nmodel = LogisticRegression\n\n# Set up class weights. See previous explanation.\nclass_weights = compute_class_weight(class_weight='balanced', classes=np.array([0, 1]), y=y_train)\nclass_weights_dict = dict(zip([0, 1], class_weights))\n\n# Fit data to model\nmodel.fit(X_train, y_train, epochs=5, batch_size=64, validation_data=(X_test, y_test), class_weight=class_weights_dict)\n\ny_pred = model.predict(X_test)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "b98f5bc2-938e-4983-a986-d9f5ae9ac51a",
      "cell_type": "code",
      "source": "# This cell is for evaluating the model\ntest_loss, test_accuracy, test_auc = model.evaluate(X_test, y_test)\ntrain_loss, train_accuracy, train_auc = model.evaluate(X_train, y_train)\nprint(f'Test accuracy: {test_accuracy:.2f}')\nprint(f'Train accuracy: {train_accuracy:.2f}')\nprint(f'Overfitting: {train_accuracy-test_accuracy:.2f}')\n\n# Convert predicted probabilities to binary classification (positive or negative review)\ny_pred = model.predict(X_test)  \n\n# Metrics\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\nroc_auc = roc_auc_score(y_test, y_pred_prob) \n\n# Results\nprint(f'Accuracy: {accuracy:.2f}')\nprint(f'Precision: {precision:.2f}') # How many predicted positives are actually positive\nprint(f'Recall: {recall:.2f}') # How many actual positive cases were correctly predicted\nprint(f'F1-Score: {f1:.2f}') # Balance betweet precision and recall\nprint(f'ROC-AUC: {roc_auc:.2f}') # Area under ROC curve (recall against false positive rate)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}